import json
import os
import getpass
import re
from datetime import datetime
from typing import List, Dict, Any

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LangChain
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é
from dotenv import load_dotenv
from getpass import getpass

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


from dataclasses import dataclass

def format_order_details(info: dict) -> dict:
    #–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –ø–æ–∫—É–ø–∞—Ç–µ–ª—é
    formatted = {}
    status = info.get("status")
    if status == "in_transit":
        eta = info.get("eta_days", 0)
        carrier = info.get("carrier", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
        detail = f"–ó–∞–∫–∞–∑ –≤ –ø—É—Ç–∏. –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ —á–µ—Ä–µ–∑ {eta} –¥–Ω. –ü–µ—Ä–µ–≤–æ–∑—á–∏–∫: {carrier}."
    elif status == "delivered":
        delivered_at = info.get("delivered_at", "–Ω–µ —É–∫–∞–∑–∞–Ω–∞")
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –º–æ–∂–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç—É –∫—Ä–∞—Å–∏–≤–æ
            date_obj = datetime.strptime(delivered_at, "%Y-%m-%d")
            delivered_at = date_obj.strftime("%d.%m.%Y")
        except ValueError:
            pass  # –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
        detail = f"–ó–∞–∫–∞–∑ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω {delivered_at}."
    elif status == "processing":
        note = info.get("note", "–ë–µ–∑ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π")
        detail = f"–ó–∞–∫–∞–∑ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ. {note}"
    else:
        detail = f"–°—Ç–∞—Ç—É—Å –∑–∞–∫–∞–∑–∞: {status}." if status else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–∫–∞–∑–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."

    return detail

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è LLM –∏ embeddings"""
    api_key: str
    base_url: str = ""
    embedding_model: str = "text-embedding-3-small"
    llm_model: str = "gpt-3.5-turbo-instruct"
    temperature: float = 0.3

def setup_api_config() -> ModelConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è -> .env —Ñ–∞–π–ª -> —Ä—É—á–Ω–æ–π –≤–≤–æ–¥"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    load_dotenv()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("LLM_API_BASE_URL", "")
    embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    llm_model = os.getenv("LLM_MODEL", "gpt-3.5-turbo-instruct")
    temperature = float(os.getenv("LLM_TEMPERATURE", "0.3"))
        
    return ModelConfig(
        api_key=api_key,
        base_url=base_url,
        embedding_model=embedding_model,
        llm_model=llm_model,
        temperature=temperature
    )

def create_vector_store(texts: List[str], config: ModelConfig) -> FAISS:
    """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–π –º–æ–¥–µ–ª—å—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ embedding –º–æ–¥–µ–ª–∏
        embedding_kwargs = {
            "api_key": config.api_key,
            "model": config.embedding_model
        }
        
        if config.base_url:
            embedding_kwargs["base_url"] = config.base_url
        
        embeddings = OpenAIEmbeddings(**embedding_kwargs)
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤
        documents = [Document(page_content=text) for text in texts]
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ (–¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π)
        text_splitter = CharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separator="\n"
        )
        docs = text_splitter.split_documents(documents)
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        vector_store = FAISS.from_documents(docs, embeddings)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
        return vector_store
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {str(e)}")
        raise



def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞
        config = setup_api_config()
        logger.info(config)
        
        llm_kwargs = {
            "api_key": config.api_key,
            "temperature": config.temperature,
            "model_name": config.llm_model
        }

        if config.base_url:
            llm_kwargs["openai_api_base"] = config.base_url

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        with open('data/faq.json', mode='r', encoding='utf-8') as file:
            faq = json.load(file)
        text_data = [f"–í–æ–ø—Ä–æ—Å:'{qa['q']}'\n–û—Ç–≤–µ—Ç:'{qa['a']}'" for qa in faq]

        #–î–∞–Ω–Ω—ã–µ –æ –∑–∞—è–≤–∫–∞—Ö
        with open('data/orders.json', mode='r', encoding="utf-8") as file:
            orders = json.load(file)
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        vector_store = create_vector_store(text_data, config)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ retriever
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 2}  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 2 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        )
        llm = ChatOpenAI(**llm_kwargs)

        # –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        qa_prompt = PromptTemplate(
            template="""
            –¢—ã ‚Äî –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –º–∞–≥–∞–∑–∏–Ω–∞ Shoply, –æ—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –≤–µ–∂–ª–∏–≤–æ. 
            –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
            –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.
            
            –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
            
            –í–æ–ø—Ä–æ—Å: {input}
            –û—Ç–≤–µ—Ç:""",
            input_variables=["context", "input"]
        )

        document_chain = create_stuff_documents_chain(llm, qa_prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        def invoke_qa(query: str):
            response = retrieval_chain.invoke({"input": query})
            return {
                "result": response["answer"],
                "source_documents": response.get("context", [])
            }
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\n" + "="*50)
        print("–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")
        print("="*50)
        
        while True:
            query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            if query.lower() in ['exit', 'quit', '–≤—ã–π—Ç–∏']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üê±")
                break
                
            if not query:
                continue

            if query.startswith("/order"):
                match = re.fullmatch(r'/order\s+(\d+)', query.strip())
                if order := orders.get(match.group(1)):
                    print(f'–ó–∞–∫–∞–∑ #{match.group(1)}: {format_order_details(order)}')
                else:
                    print('–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.')
                continue
                
            try:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
                response = invoke_qa(query)
                print("\n–û—Ç–≤–µ—Ç:", response["result"])
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
                print("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
                
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        exit(1)

if __name__ == "__main__":
    main()