import argparse
import json

import logging
import os
import re
import sys
import traceback
from datetime import datetime
from typing import List, Dict, Any, Union

from dotenv import load_dotenv
from langchain.callbacks import get_openai_callback

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

from dataclasses import dataclass


@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è LLM –∏ embeddings"""
    api_key: str
    base_url: str = ""
    embedding_model: str = "text-embedding-3-small"
    llm_model: str = "gpt-4o-mini"
    temperature: float = 0.3
    context_length: int = 3
    brand_name: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Å–ª–æ–≤–∞—Ä—å —Å –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π API –∫–ª—é—á–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            –°–ª–æ–≤–∞—Ä—å —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "embedding_model": self.embedding_model,
            "llm_model": self.llm_model,
            "temperature": self.temperature,
            "context_length": self.context_length
        }


def setup_api_config() -> ModelConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è -> .env —Ñ–∞–π–ª -> —Ä—É—á–Ω–æ–π –≤–≤–æ–¥"""
    # –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    load_dotenv()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("LLM_API_BASE_URL")
    embedding_model = os.getenv("EMBEDDING_MODEL")
    llm_model = os.getenv("LLM_MODEL")
    temperature = float(os.getenv("LLM_TEMPERATURE"))
    context_length = int(os.getenv("CONTEXT_LENGTH"))
    brand_name = os.getenv("BRAND_NAME")

    return ModelConfig(
        api_key=api_key,
        base_url=base_url,
        embedding_model=embedding_model,
        llm_model=llm_model,
        temperature=temperature,
        context_length=context_length,
        brand_name=brand_name
    )


class Consultant:
    def __init__(self, model_config: ModelConfig):
        self.model_config = model_config

        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(script_dir, "data")
        with open(os.path.join(data_dir, "faq.json"), mode='r', encoding='utf-8') as file:
            self.faq = json.load(file)
        with open(os.path.join(data_dir, "orders.json"), mode='r', encoding="utf-8") as file:
            self.orders = json.load(file)

        os.makedirs("logs", exist_ok=True)
        now: str = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.logger = self.setup_logger(f"logs/session_{now}.jsonl")

        self.conversation_history: list[dict[str, str]] = []
        self.context_length = self.model_config.context_length

    @staticmethod
    def setup_logger(log_file):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)

        handler = logging.FileHandler(log_file, encoding="utf-8")
        handler.setFormatter(logging.Formatter("%(message)s"))
        logger.addHandler(handler)

        return logger

    def format_conversation_history(self) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ —Å—Ç—Ä–æ–∫—É"""
        return "\n".join(
            [f"{msg['role'].upper()}: {msg['content']}"
             for msg in self.conversation_history[-self.context_length:]]
        )

    def add_to_history(self, role: str, content: str):
        self.conversation_history.append({"role": role, "content": content})

    def prepare_text_faq(self) -> List[str]:
        return [f"–í–æ–ø—Ä–æ—Å:'{qa['q']}'\n–û—Ç–≤–µ—Ç:'{qa['a']}'" for qa in self.faq]

    def create_vector_store(self) -> FAISS:
        texts = self.prepare_text_faq()

        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ embedding –º–æ–¥–µ–ª–∏
            embedding_kwargs = {
                "api_key": self.model_config.api_key,
                "model": self.model_config.embedding_model
            }

            if self.model_config.base_url:
                embedding_kwargs["base_url"] = self.model_config.base_url

            embeddings = OpenAIEmbeddings(**embedding_kwargs)

            documents = [Document(page_content=text) for text in texts]

            text_splitter = CharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separator="\n"
            )
            docs = text_splitter.split_documents(documents)

            vector_store = FAISS.from_documents(docs, embeddings)
            self.add_log(event='vector_store_created', document_count=len(docs), chunks_count=len(docs))
            return vector_store

        except Exception as e:
            details = {
                    "documents_count": len(texts) if 'texts' in locals() else 0
                }
            self.add_log(type='error', message=str(e), details=details, event_type='vector_store_creation')
            raise

    def retrieval_chain(self, model: ChatOpenAI, vector_store: FAISS):
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        qa_prompt = PromptTemplate(
            template="""
                    –¢—ã ‚Äî –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –º–∞–≥–∞–∑–∏–Ω–∞ {brand_name}, –æ—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –≤–µ–∂–ª–∏–≤–æ. 
                    –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
                    –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.

                    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
                    
                    –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞: 
                    {history}

                    –í–æ–ø—Ä–æ—Å: {input}
                    –û—Ç–≤–µ—Ç:""",
            input_variables=["brand_name", "context", "input", "history"]
        )
        document_chain = create_stuff_documents_chain(model, qa_prompt)
        return create_retrieval_chain(retriever, document_chain)

    def faq_processor(self, query: str, retrieval_chain):
        try:
            with get_openai_callback() as cb:
                response = retrieval_chain.invoke({
                    "brand_name": self.model_config.brand_name,
                    "input": query,
                    "history": self.format_conversation_history()
                })
                serializable_response = {
                    "answer": response.get("answer", ""),
                    "usage": {"total_tokens": cb.total_tokens, "prompt_tokens": cb.prompt_tokens, "completion_tokens": cb.completion_tokens},
                    "context": ""
                }

            self.add_log(query=query, message=serializable_response)
            return response["answer"]
        except Exception as e:
            self.add_log(type='error', query=query, message=str(e), event='faq_error', event_type='faq_processing')
            print("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")

    def orders_processor(self, query: str):
        match = re.fullmatch(r'/order\s+(\d+)', query.strip())
        if not match:
            response = '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /order <–Ω–æ–º–µ—Ä>'
            self.add_log(type='error', query=query, message=response, event='order_error', event_type='invalid_format')
            return response

        order_id = match.group(1)
        if order := self.orders.get(order_id):
            response = f'–ó–∞–∫–∞–∑ #{match.group(1)}: {format_order_details(order)}'
            self.add_log(query=query, message=response)
            return response
        else:
            response = '–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.'
            self.add_log(type='error', query=query, message=response, event='order_error', event_type='not_found')
            return response

    def add_log(self, type: str = "info", query: str = None, message: Union[str, dict] = None, **kwargs):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "message": message,
            **kwargs
        }
        if type == 'error':
            self.logger.error(json.dumps(log_entry, ensure_ascii=False))
        else:
            self.logger.info(json.dumps(log_entry, ensure_ascii=False))


def format_order_details(info: dict) -> str:
    status = info.get("status")
    if status == "in_transit":
        eta = info.get("eta_days", 0)
        carrier = info.get("carrier", "–Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")
        detail = f"–ó–∞–∫–∞–∑ –≤ –ø—É—Ç–∏. –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ —á–µ—Ä–µ–∑ {eta} –¥–Ω. –ü–µ—Ä–µ–≤–æ–∑—á–∏–∫: {carrier}."
    elif status == "delivered":
        delivered_at = info.get("delivered_at", "–Ω–µ —É–∫–∞–∑–∞–Ω–∞")
        try:
            # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –º–æ–∂–Ω–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç—É –∫—Ä–∞—Å–∏–≤–æ
            date_obj = datetime.strptime(delivered_at, "%Y-%m-%d")
            delivered_at = date_obj.strftime("%d.%m.%Y")
        except ValueError:
            pass  # –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
        detail = f"–ó–∞–∫–∞–∑ –¥–æ—Å—Ç–∞–≤–ª–µ–Ω {delivered_at}."
    elif status == "processing":
        note = info.get("note", "–ë–µ–∑ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π")
        detail = f"–ó–∞–∫–∞–∑ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ. {note}"
    else:
        detail = f"–°—Ç–∞—Ç—É—Å –∑–∞–∫–∞–∑–∞: {status}." if status else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–∫–∞–∑–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."

    return detail


def main():
    parser = argparse.ArgumentParser(description="Consultant Bot")
    parser.add_argument('--url', type=str, help='Base URL for LLM API')
    parser.add_argument('--model', type=str, help='LLM model name')
    parser.add_argument('--api-key', type=str, help='API key for authentication')
    args = parser.parse_args()

    try:
        model_config = setup_api_config()
        if args.api_key:
            model_config.api_key = args.api_key
        if args.url:
            model_config.base_url = args.url
        if args.model:
            model_config.llm_model = args.model

        bot = Consultant(model_config=model_config)
        config = bot.model_config
        bot.add_log(event="config_loaded", config=config.to_dict())

        llm_kwargs = {
            "api_key": config.api_key,
            "temperature": config.temperature,
            "model_name": config.llm_model,
            "openai_api_base": config.base_url}
        model = ChatOpenAI(**llm_kwargs)

        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        vector_store = bot.create_vector_store()
        retrieval_chain = bot.retrieval_chain(model=model, vector_store=vector_store)

        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\n" + "=" * 50)
        print("–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

        while True:
            query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            bot.add_to_history("user", query)
            if query.lower() in ['exit', 'quit', '–≤—ã–π—Ç–∏']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! üê±")
                bot.add_log(message="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–ª –≤—ã—Ö–æ–¥.")
                break

            if not query:
                continue

            if query.startswith("/order"):
                response = bot.orders_processor(query=query)
                print(response)
                bot.add_to_history("assistant", response)
                continue

            response = bot.faq_processor(query=query, retrieval_chain=retrieval_chain)
            print(response)
            bot.add_to_history("assistant", response)

    except Exception as e:
        error_entry = {
            "timestamp": datetime.now().isoformat(),
            "event": "critical_error",
            "error": str(e),
            "traceback": traceback.format_exc() if 'traceback' in sys.modules else None
        }
        logging.getLogger(__name__).error(json.dumps(error_entry, ensure_ascii=False))
        print("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è. –î–µ—Ç–∞–ª–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ –ª–æ–≥.")
        exit(1)


if __name__ == "__main__":
    main()
